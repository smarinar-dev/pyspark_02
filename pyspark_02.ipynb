{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bba457",
   "metadata": {},
   "source": [
    "# Understanding SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python list of numbers from 1 to 100 \n",
    "numb = range(1, 100)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local file into PySpark shell\n",
    "lines = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb963ea",
   "metadata": {},
   "source": [
    "### general sintax of lambda function: `lambda arguments: expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb390396",
   "metadata": {},
   "outputs": [],
   "source": [
    "double lambda: x: x*2\n",
    "print(double(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63182b",
   "metadata": {},
   "source": [
    "### lambda-map: \n",
    "`map()` applies a function to all items in a list\n",
    "\n",
    "`map(function, list)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6619211",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(map(lambda x: x+2, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fca812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print my_list in the console\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "# Square all numbers in my_list\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "# Print the result of the map function\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb992f3d",
   "metadata": {},
   "source": [
    "### lambda-map: \n",
    "`filter()` takes a function and a list and returns a new list for which the function returns `True`.\n",
    "\n",
    "`filter(function, list)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa89f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1, 2, 3, 4]\n",
    "list(filter(lambda x: x%2 != 0, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print my_list2 in the console\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "# Filter numbers divisible by 10\n",
    "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
    "\n",
    "# Print the numbers divisible by 10\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff560dfc",
   "metadata": {},
   "source": [
    "# RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec413fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3284b50",
   "metadata": {},
   "source": [
    "## Operations to RDDs: Transformations and Actions\n",
    "\n",
    "* Transformations create new RDDs\n",
    "* Actions perform computations on RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac490d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73567f87",
   "metadata": {},
   "source": [
    "# Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94487c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "  print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e11567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 words and their frequencies from the input RDD\n",
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "\n",
    "# Swap the keys and values from the input RDD\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(\"{},{}\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9c461",
   "metadata": {},
   "source": [
    "# PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fa752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c84bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and print their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7850c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c19fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
    "\n",
    "# Count the number of rows in both people_df_female and people_male_df DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Show the first 10 observations\n",
    "fifa_df.show(10)\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddee3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202875bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "# Plot the 'Age' density of Germany Players\n",
    "fifa_df_germany_age_pandas.plot(kind='density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9f864",
   "metadata": {},
   "source": [
    "# PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into RDD\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD \n",
    "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbf3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Return the first 2 rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ratings data\n",
    "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411941f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(file_path_spam)\n",
    "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HashingTF instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.join(non_spam_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b498e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "labels_and_preds.take(10)\n",
    "\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into an RDD\n",
    "clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with clusters from 13 to 16 and compute WSSSE\n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
    "\n",
    "# Train the model again with the best k\n",
    "model = KMeans.train(rdd_split_int, k=16, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rdd_split_int RDD into Spark DataFrame and then to Pandas DataFrame\n",
    "rdd_split_int_df_pandas = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"]).toPandas()\n",
    "\n",
    "# Convert cluster_centers to a pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# Create an overlaid scatter plot of clusters and centroids\n",
    "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a570f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
